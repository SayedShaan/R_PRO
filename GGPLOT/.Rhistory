library(DBI)
library(DBI)
con =select * from HumanResources.Employee
install.packages("odbc")
install.packages("DBI")
library(odbc)
odbcListDrivers()
con <- dbConnect(odbc(),
Driver   = "ODBC Driver 17 for SQL Server",  # or your available driver
Server   = "localhost",                     # or your server's name/IP
Database = "AdventureWorks",
UID      = "sa",                  # Remove if using Trusted Connection
PWD      = "faculty",                  # Remove if using Trusted Connection
Port     = 1433)
result <- dbGetQuery(con, "SELECT @@VERSION")
print(result)
data <- dbReadTable(con, "Employees")
data <- dbReadTable(con, "AdventureWorks.humanresource.Employees")
print(result)
data <- dbReadTable(con, "humanresource.Employees")
dbListTables(con)
data <- dbReadTable(con, "Employee")
# Create the connection
con <- dbConnect(odbc(),
Driver   = "ODBC Driver 17 for SQL Server",
Server   = "localhost",
Database = "AdventureWorks",
Trusted_Connection = "Yes")
employee_data <- dbGetQuery(con, "SELECT * FROM HumanResources.Employee")
employee_data
head(employee_data)
employee_data <- dbGetQuery(con, "SELECT * FROM HumanResources.Employee")
con <- dbConnect(odbc(),
Driver   = "ODBC Driver 17 for SQL Server",
Server   = "localhost",
Database = "AdventureWorks",
Trusted_Connection = "Yes")
install.packages("odbc")
install.packages("DBI")
library(odbc)
odbcListDrivers()
con <- dbConnect(odbc(),
Driver   = "ODBC Driver 17 for SQL Server",  # or your available driver
Server   = "localhost",                     # or your server's name/IP
Database = "AdventureWorks",
UID      = "sa",                  # Remove if using Trusted Connection
PWD      = "faculty",                  # Remove if using Trusted Connection
Port     = 1433)
con <- dbConnect(odbc(),
Driver   = "ODBC Driver 17 for SQL Server",
Server   = "localhost",
Database = "AdventureWorks",
Trusted_Connection = "Yes")
employee_data <- dbGetQuery(con, "SELECT * FROM HumanResources.Employee")
employee_data
employee_data <- dbGetQuery(con, "SELECT * FROM HumanResources.department")
employee_data
employee_data <- dbGetQuery(con, "SELECT top 10 FROM HumanResources.employee")
employee_data <- dbGetQuery(con, "SELECT top 10(*) FROM HumanResources.employee")
employee_data <- dbGetQuery(con, "SELECT * FROM HumanResources.employee")
employee_data
employee_data <- dbGetQuery(con, "SELECT top 10 FROM HumanResources.employee")
employee_data
employee_data <- "SELECT DISTINCT TOP(100)  FROM HumanResources.employee ;"
employee_data
employee_data <- "SELECT DISTINCT TOP(100)  FROM HumanResources.employee ;"
employee_data
source("~/.active-rstudio-document", echo = TRUE)
g + geom_histogram(aes(fill=class),
bins=5,
col="black",
size=.1) +   # change number of bins
labs(title="Histogram with Fixed Bins",
subtitle="Engine Displacement across Vehicle Classes")
---
title: "New"
library(ggplot2)
library(dplyr)
# Create a sample data frame
data <- data.frame(
Name = c("Amit", "Priya", "Rohan", "Sana"),
Score = c(82, 90, 76, 88)
)
# View data
data
# Calculate average score
mean(data$Score)
# Bar plot of Scores
ggplot(data, aes(x = Name, y = Score, fill = Name)) +
geom_bar(stat = "identity") +
theme_minimal() +
setwd("D:/SHAAN/R file")
setwd("D:/SHAAN/R file/WEB Scraping")
install.packages("rvest")
library(rvest)
single_table_page <- read_html("first_page.html")
single_table_page
html_table(single_table_page)
html_table(html_node(single_table_page,"table"))
html_table(html_nodes(single_table_page,"table"))
single_table_page %>% # single records from the webpage
html_node("table") %>%
html_table()
single_table_page %>%    # all the records for the webpage
html_nodes("table") %>%
html_table()
product_page <- read_html("products.html")
names <- product_page %>%
html_nodes("span.name") %>%
html_text()
prices <- product_page %>%
html_nodes("span.price") %>%
html_text()
names
prices
myproduct <- data.frame(
names=names,
prices=prices
)
myproduct
products_page <- read_html("product.html")
products_page %>%
html_nodes(xpath = "//div[@class='product-list']//li//span[@class='names']//span[@class='prices']")
html_text()
document <- read_html("https://scrapingcourse.com/ecommerce/")
html_products <- document %>% html_elements("li.product")
a_element <- html_products %>% html_element("a")
img_element <- html_products %>% html_element("img")
h2_element <- html_products %>% html_element("h2")
span_element <- html_products %>% html_element("span")
product_urls <- html_products %>%
html_element("a") %>%
html_attr("href")
product_images <- html_products %>%
html_element("img") %>%
html_attr("src")
product_names <- html_products %>%
html_element("h2") %>%
html_text2()
product_prices <- html_products %>%
html_element("span") %>%
html_text2()
products <- data.frame(
product_urls,
product_images,
product_names,
product_prices
)
products
names(products) <- c("url", "image", "name", "price")
write.csv(products, file = "./products.csv", fileEncoding = "UTF-8")
library(tidyr)
storms <- data.frame(
"storms" = c("Alberto", "Alex", "Ana", "Arlene", "Arthur"),
"wind" = c(110, 45, 65, 40, 30),
"Pressure" = c(1007, 1008, 1006, 1005, 1007),
"date" = c("2001-08-12", "2002-07-02", "2000-07-11", "2004-06-21", "2001-06-13")
)
case <- data.frame(
"country" = c("FR", "DE", "US"),
"1995" = c(7000, 58000, 15000),
"1996" = c(4000, 56000, 14000),
"1997" = c(6000, 54000, 17000)
)
pollution <- data.frame(
"city" = c("New York", "New York", "London", "London", "Beijing", "Beijing"),
"pratical.size" = c("large", "small", "large", "small", "large", "small"),
"amount" = c(23, 14, 28, 12, 110, 58)
)
storms
case
pollution
df <- tidyr::gather(case, "Year", "n", 2:4)
df
pollution
tidyr::spread(pollution,"pratical.size",amount)
storms
storms2 <- tidyr::separate(storms, date = c("Year", "Month", "Date"), sep = "-")
storms
storms2 <- tidyr::separate(storms, date = c("Year", "Month", "Date"), sep = "-")
storms2 <- tidyr::separate(storms, col = "Year", into = c("Year", "Month", "Date"), sep = "-")
storms2 <- tidyr::separate(storms, col = "Year", into = c("Year", "Month", "Date"), sep = "-")
install.packages("tidyr",dependencies = T,repos = "https://cran.r-project.org/")
library(tidyr)
storms2 <- tidyr::separate(storms, date = c("Year", "Month", "Date"), sep = "-")
storms <- data.frame(
"storms" = c("Alberto", "Alex", "Ana", "Arlene", "Arthur"),
"wind" = c(110, 45, 65, 40, 30),
"Pressure" = c(1007, 1008, 1006, 1005, 1007),
"date" = c("2001-08-12", "2002-07-02", "2000-07-11", "2004-06-21", "2001-06-13")
)
case <- data.frame(
"country" = c("FR", "DE", "US"),
"1995" = c(7000, 58000, 15000),
"1996" = c(4000, 56000, 14000),
"1997" = c(6000, 54000, 17000)
)
pollution <- data.frame(
"city" = c("New York", "New York", "London", "London", "Beijing", "Beijing"),
"pratical.size" = c("large", "small", "large", "small", "large", "small"),
"amount" = c(23, 14, 28, 12, 110, 58)
)
storms
storms2 <- tidyr::separate(storms, date = c("Year", "Month", "Date"), sep = "-")
storms2 <- tidyr::separate(storms, col = "Year", into = c("Year", "Month", "Date"), sep = "-")
storms["ratio"]=storms$Pressure/storms$wind
storms
tidyr::unite(storms2,date,Year,Month,Date,sep = "/")
tidyr::unite(storms,date,Year,Month,Date,sep = "/")
install.package("devtools",dependencies = T,respos= "https://cran.r-project.org/")
devtools::install_github("https://github.com/rstudio/EDAWR")
library(devtools)
devtools::install_github("rstudio/EDAWR")
library(EDAWR)
storms
rm(storms,case,pollution)
storms
storms2 <- tidyr::separate(storms, date = c("Year", "Month", "Date"), sep = "-")
storms2 <- tidyr::separate(storms, col = "Year", into = c("Year", "Month", "Date"), sep = "-")
storms
storms2 <- tidyr::separate(storms, col = "date", into = c("Year", "Month", "Date"), sep = "-")
storms2
tidyr::unite(storms2,date,Year,Month,Date,sep = "/")
df <- tidyr::gather(case, "Year", "n", 2:4)
case <- data.frame(
"country" = c("FR", "DE", "US"),
"1995" = c(7000, 58000, 15000),
"1996" = c(4000, 56000, 14000),
"1997" = c(6000, 54000, 17000)
)
df <- tidyr::gather(case, "Year", "n", 2:4)
df
df[df$Year == 'X1995', ]
nba <- data.frame(
player = c("James","Durant","Curry","Harden","Paul","Wade"),
team = c("CLEOH","GSWOAK","GSWOAK","HOUTX","HOUTX","CLEOH"),
day1point = c("25","23","30","41","26","20"),
day2point = c("24","25","33","45","26","23")
)
nba
newnba <- tidyr::gather(nba,"point","n",3:4)
newnba
newnba <- tidyr::spread(newnba,"point","n")
newnba
newnba[newnba$point=="day1point",]
newnba
seperated <- tidyr::separate(nba,team,c("team","state"),sep = 3)
seperated
tidyr::unite(seperated,"team",team,state,sep ="-")
library(rvest)
single_table_page <- read_html("first_page.html")
single_table_page
html_table(single_table_page)
html_table(html_node(single_table_page,"table"))
html_table(html_nodes(single_table_page,"table"))
html_table(single_table_page)
single_table_page %>% # single records from the webpage
html_node("table") %>%
html_table()
single_table_page %>%    # all the records for the webpage
html_nodes("table") %>%
html_table()
product_page <- read_html("products.html")
names <- product_page %>%
html_nodes("span.name") %>%
html_text()
prices <- product_page %>%
html_nodes("span.price") %>%
html_text()
names
prices
myproduct <- data.frame(
names=names,
prices=prices
)
myproduct
document <- read_html("https://scrapingcourse.com/ecommerce/")
html_products <- document %>% html_elements("li.product")
html_products <- document %>% html_elements("li.product")
a_element <- html_products %>% html_element("a")
img_element <- html_products %>% html_element("img")
h2_element <- html_products %>% html_element("h2")
span_element <- html_products %>% html_element("span")
product_urls <- html_products %>%
html_element("a") %>%
html_attr("href")
product_images <- html_products %>%
html_element("img") %>%
html_attr("src")
product_names <- html_products %>%
html_element("h2") %>%
html_text2()
product_prices <- html_products %>%
html_element("span") %>%
html_text2()
products <- data.frame(
product_urls,
product_images,
product_names,
product_prices
)
products
names(products) <- c("url", "image", "name", "price")
setwd("E:/Shaan/R file/WEB Scraping")
install.packages("rvest")
library(rvest)
single_table_page <- read_html("first_page.html")
single_table_page
html_table(single_table_page)
html_table(html_node(single_table_page,"table"))
html_table(html_nodes(single_table_page,"table"))
single_table_page %>% # single records from the webpage
html_node("table") %>%
html_table()
single_table_page %>%    # all the records for the webpage
html_nodes("table") %>%
html_table()
product_page <- read_html("products.html")
names <- product_page %>%
html_nodes("span.name") %>%
html_text()
prices <- product_page %>%
html_nodes("span.price") %>%
html_text()
names
prices
myproduct <- data.frame(
names=names,
prices=prices
)
myproduct
products_page <- read_html("product.html")
products_page %>%
html_nodes(xpath = "//div[@class='product-list']//li//span[@class='names']//span[@class='prices']")
html_text()
myproduct
document <- read_html("https://scrapingcourse.com/ecommerce/")
html_products <- document %>% html_elements("li.product")
a_element <- html_products %>% html_element("a")
img_element <- html_products %>% html_element("img")
h2_element <- html_products %>% html_element("h2")
span_element <- html_products %>% html_element("span")
product_urls <- html_products %>%
html_element("a") %>%
html_attr("href")
product_images <- html_products %>%
html_element("img") %>%
html_attr("src")
product_names <- html_products %>%
html_element("h2") %>%
html_text2()
product_prices <- html_products %>%
html_element("span") %>%
html_text2()
products <- data.frame(
product_urls,
product_images,
product_names,
product_prices
)
products
setwd("E:/Shaan/R file/Charts")
setwd("E:/Shaan/R file/GGPLOT")
install.packages("ggplot2")
library("ggplot2")
table(iris)
ggplot(data=iris,
aes(y=Sepal.Length,
x=Petal.Length,
col=Species))+ geom_point()
iris
data <- c(75000,45000)
data <- c(75000,45000)
gender <- c("Male","Female")
barplot(data,
names.arg = gender,
col = "red")
barplot(data,
names.arg = gender,
col = c("red","blue"),
barplot(data,
names.arg = gender,
col = c("red","blue")
barplot(data,
barplot(data,
names.arg = gender,
col = c("red","blue"),
horiz = T)
